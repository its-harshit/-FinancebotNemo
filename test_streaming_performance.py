#!/usr/bin/env python3
"""
Performance test for streaming optimization
Tests the improved streaming configuration against NeMo Guardrails standards
"""

import asyncio
import time
from finance_bot import NPCIGrievanceBot

async def test_streaming_performance():
    """Test streaming performance with optimizations."""
    print("ğŸš€ Testing Optimized Streaming Performance")
    print("=" * 60)
    
    bot = NPCIGrievanceBot()
    
    # Test messages of varying complexity
    test_cases = [
        "My UPI payment failed",
        "My UPI payment failed but money was debited from my account. Transaction reference: 304912345678",
        "I have a complex issue with my NACH mandate where the auto-debit failed for my EMI payment and I need immediate assistance with escalation",
    ]
    
    for i, test_message in enumerate(test_cases, 1):
        print(f"\nğŸ“Š Test Case {i}: {test_message[:50]}{'...' if len(test_message) > 50 else ''}")
        print("-" * 60)
        
        # Measure streaming performance
        start_time = time.time()
        first_chunk_time = None
        chunk_count = 0
        total_chars = 0
        
        print("Response: ", end="", flush=True)
        
        try:
            async for chunk in bot.stream_message(test_message, f"perf_test_user_{i}"):
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                    
                print(chunk, end="", flush=True)
                chunk_count += 1
                total_chars += len(chunk)
            
            end_time = time.time()
            
            # Performance metrics
            total_time = end_time - start_time
            time_to_first_chunk = (first_chunk_time - start_time) if first_chunk_time else total_time
            
            print(f"\n\nğŸ“ˆ Performance Metrics:")
            print(f"   â±ï¸  Time to first chunk: {time_to_first_chunk:.3f}s")
            print(f"   â±ï¸  Total response time: {total_time:.3f}s")
            print(f"   ğŸ“¦ Total chunks: {chunk_count}")
            print(f"   ğŸ“ Total characters: {total_chars}")
            print(f"   ğŸ”„ Chars/second: {total_chars/total_time:.1f}")
            
            # Performance assessment
            if time_to_first_chunk < 0.5:
                print(f"   âœ… Excellent first chunk latency!")
            elif time_to_first_chunk < 1.0:
                print(f"   ğŸ‘ Good first chunk latency")
            else:
                print(f"   âš ï¸  High first chunk latency - needs optimization")
                
        except Exception as e:
            print(f"\nâŒ Error: {e}")

async def test_non_streaming_comparison():
    """Compare with non-streaming performance."""
    print(f"\nğŸ”„ Non-Streaming Comparison")
    print("=" * 60)
    
    bot = NPCIGrievanceBot()
    test_message = "My UPI payment failed but money was debited"
    
    print("Non-streaming response:")
    start_time = time.time()
    
    try:
        response = await bot.process_message(test_message, "non_stream_user")
        end_time = time.time()
        
        non_streaming_time = end_time - start_time
        response_text = response.get("response", "No response")
        
        print(f"Response: {response_text}")
        print(f"\nğŸ“Š Non-streaming metrics:")
        print(f"   â±ï¸  Total time: {non_streaming_time:.3f}s")
        print(f"   ğŸ“ Response length: {len(response_text)} characters")
        
        return non_streaming_time
        
    except Exception as e:
        print(f"âŒ Non-streaming error: {e}")
        return 0

async def benchmark_streaming_optimizations():
    """Benchmark the streaming optimizations."""
    print(f"\nğŸ† Streaming Optimization Benchmark")
    print("=" * 60)
    
    bot = NPCIGrievanceBot()
    test_message = "Help me with a UPI transaction failure where money was debited but payment failed"
    
    # Test multiple runs for consistency
    runs = 3
    first_chunk_times = []
    total_times = []
    
    for run in range(runs):
        print(f"\nRun {run + 1}/{runs}: ", end="", flush=True)
        
        start_time = time.time()
        first_chunk_time = None
        chunk_count = 0
        
        try:
            async for chunk in bot.stream_message(test_message, f"benchmark_user_{run}"):
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                print(".", end="", flush=True)
                chunk_count += 1
            
            end_time = time.time()
            
            time_to_first = (first_chunk_time - start_time) if first_chunk_time else 0
            total_time = end_time - start_time
            
            first_chunk_times.append(time_to_first)
            total_times.append(total_time)
            
            print(f" âœ… ({time_to_first:.3f}s to first chunk)")
            
        except Exception as e:
            print(f" âŒ Error: {e}")
    
    if first_chunk_times:
        avg_first_chunk = sum(first_chunk_times) / len(first_chunk_times)
        avg_total = sum(total_times) / len(total_times)
        
        print(f"\nğŸ“Š Average Performance (over {runs} runs):")
        print(f"   â±ï¸  Average time to first chunk: {avg_first_chunk:.3f}s")
        print(f"   â±ï¸  Average total time: {avg_total:.3f}s")
        
        # Performance targets based on NeMo Guardrails standards
        print(f"\nğŸ¯ Performance Assessment:")
        if avg_first_chunk < 0.5:
            print(f"   âœ… EXCELLENT - Meets high-performance streaming standards")
        elif avg_first_chunk < 1.0:
            print(f"   ğŸ‘ GOOD - Meets standard streaming performance")
        elif avg_first_chunk < 2.0:
            print(f"   âš ï¸  ACCEPTABLE - Could benefit from further optimization")
        else:
            print(f"   âŒ POOR - Requires optimization")

async def test_streaming_with_context():
    """Test streaming performance with conversation context."""
    print(f"\nğŸ§  Context-Aware Streaming Performance")
    print("=" * 60)
    
    bot = NPCIGrievanceBot()
    
    # Build conversation history
    conversation_history = [
        {"role": "user", "content": "I have a UPI issue"},
        {"role": "assistant", "content": "I can help with UPI issues. What specific problem are you facing?"},
        {"role": "user", "content": "Payment failed but money was debited"},
        {"role": "assistant", "content": "I understand. This is a common UPI issue. Let me help you resolve this."}
    ]
    
    follow_up = "It was for 500 rupees and the UPI Ref ID is 304912345678"
    
    print(f"Follow-up with context: {follow_up}")
    print("Response: ", end="", flush=True)
    
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    
    try:
        async for chunk in bot.stream_message(follow_up, "context_user", conversation_history):
            if first_chunk_time is None:
                first_chunk_time = time.time()
            print(chunk, end="", flush=True)
            chunk_count += 1
        
        end_time = time.time()
        
        time_to_first = (first_chunk_time - start_time) if first_chunk_time else 0
        total_time = end_time - start_time
        
        print(f"\n\nğŸ“Š Context Streaming Performance:")
        print(f"   â±ï¸  Time to first chunk: {time_to_first:.3f}s")
        print(f"   â±ï¸  Total time: {total_time:.3f}s")
        print(f"   ğŸ“¦ Chunks: {chunk_count}")
        print(f"   ğŸ§  Context messages: {len(conversation_history)}")
        
    except Exception as e:
        print(f"\nâŒ Context streaming error: {e}")

async def main():
    """Run all performance tests."""
    print("ğŸ›ï¸  NPCI Grievance Bot - Streaming Performance Test")
    print("Testing optimizations based on NeMo Guardrails standards")
    print("=" * 60)
    
    try:
        await test_streaming_performance()
        non_streaming_time = await test_non_streaming_comparison()
        await benchmark_streaming_optimizations()
        await test_streaming_with_context()
        
        print(f"\nğŸ‰ Performance Testing Complete!")
        print(f"\nğŸ“‹ Optimization Summary:")
        print(f"   âœ… Streaming configuration optimized")
        print(f"   âœ… Output rails streamlined for streaming")
        print(f"   âœ… Parallel rail execution enabled")
        print(f"   âœ… Chunk size optimized (50 chars)")
        print(f"   âœ… Context window optimized (8 messages)")
        
        print(f"\nğŸ”§ Key Optimizations Applied:")
        print(f"   â€¢ streaming.chunk_size: 50")
        print(f"   â€¢ streaming.context_size: 20")
        print(f"   â€¢ rails.input.parallel: true")
        print(f"   â€¢ rails.output.parallel: true")
        print(f"   â€¢ output_rails_per_chunk: false")
        print(f"   â€¢ Lightweight compliance checks during streaming")
        
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ Performance test interrupted by user.")
    except Exception as e:
        print(f"\nâŒ Performance test error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
